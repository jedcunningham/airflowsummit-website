[{"content":"## The story behind the Airflow Breeze tool\nInitially, we started contributing to this fantastic open-source project [Apache Airflow] with a team of three which then grew to five. When we kicked it off a year ago, I realized pretty soon where the biggest bottlenecks and areas for improvement in terms of productivity were. Even with the help of our client, who provided us with a “homegrown” development environment it took us literally days to set it up and learn some basics.\n\nThat is how the journey to increased productivity in Apache Airflow began. The result? The Airflow Breeze open-source tool. Jarek Potiuk, an Airflow Committer, will tell you all about it.\n\nYou can learn [how and why it’s a \"Breeze\" to Develop Apache Airflow on Polidea blog](https://www.polidea.com/blog/its-a-breeze-to-develop-apache-airflow/?utm_source=ApacheAirflowBlog&utm_medium=Npaid&utm_campaign=Blog&utm_term=Article&utm_content=AAB_NOP_BLG_ART_AB_001).\n","url":"Its-a-breeze-to-develop-apache-airflow","title":"It's a \"Breeze\" to develop Apache Airflow","linkTitle":"It's a \"Breeze\" to develop Apache Airflow","author":"Jarek Potiuk","twitter":"higrys","github":"potiuk","linkedin":"jarekpotiuk","description":"A Principal Software Engineer's journey to developer productivity. Learn how Jarek and his team speeded up and simplified Airflow development for the community.","tags":["Development"],"date":"2019-11-22T00:00:00.000Z"},{"content":"","url":"_index","title":"News","linkTitle":"News"},{"content":"# Apache Airflow Survey 2019\n\nApache Airflow is [growing faster than ever](https://www.astronomer.io/blog/why-airflow/).\nThus, receiving and adjusting to our users’ feedback is a must. We created\n[survey](https://forms.gle/XAzR1pQBZiftvPQM7) and we got **308** responses.\nLet’s see who Airflow users are, how they play with it, and what they miss.\n\n# Overview of the user\n\n**What best describes your current occupation?**\n\n|                         |No.|  %   |\n|-------------------------|---|------|\n|Data Engineer            |194|62.99%|\n|Developer                | 34|11.04%|\n|Architect                | 23|7.47% |\n|Data Scientist           | 19|6.17% |\n|Data Analyst             | 13|4.22% |\n|DevOps                   | 13|4.22% |\n|IT Administrator         |  2|0.65% |\n|Machine Learning Engineer|  2|0.65% |\n|Manager                  |  2|0.65% |\n|Operations               |  2|0.65% |\n|Chief Data Officer       |  1|0.32% |\n|Engineering Manager      |  1|0.32% |\n|Intern                   |  1|0.32% |\n|Product owner            |  1|0.32% |\n|Quant                    |  1|0.32% |\n\n\n**In your day to day job, what do you use Airflow for?**\n\n|                                                      |No.|  %   |\n|------------------------------------------------------|---|------|\n|Data processing (ETL)                                 |298|96.75%|\n|Artificial Intelligence and Machine Learning Pipelines| 90|29.22%|\n|Automating DevOps operations                          | 64|20.78%|\n\nAccording to the survey, most of the Airflow users are the “data” people. Moreover,\n28.57% uses Airflow to both ETL and ML pipelines meaning that those two fields\nare somehow connected. Only five respondents use Airflow for DevOps operations only,\nThat means that other 59 people who use Airflow for DevOps stuff use it also for\nETL / ML  purposes.\n\n**How many active DAGs do you have in your largest Airflow instance?**\n\n|       |No.|  %   |\n|-------|---|------|\n|0-20   |115|37.34%|\n|21-40  | 65|21.10%|\n|41-60  | 44|14.29%|\n|61-100 | 28|9.09% |\n|101-200| 28|9.09% |\n|201-300|  7|2.27% |\n|301-999|  8|2.60% |\n|1000+  | 13|4.22% |\n\n\nThe majority of users do not exceed 100 active DAGs per Airflow instance. However,\nas we can see there are users who exceed thousands of DAGs with a maximum number 5000.\n\n**What is the maximum number of tasks that you have used in one DAG?**\n\n|       |No.|  %   |\n|-------|---|------|\n|0-10   | 61|19.81%|\n|11-20  | 60|19.48%|\n|21-30  | 31|10.06%|\n|31-40  | 21|6.82% |\n|41-50  | 26|8.44% |\n|51-100 | 36|11.69%|\n|101-200| 28|9.09% |\n|201-500| 21|6.82% |\n|501+   | 24|11.54%|\n\n\nThe given maximum number of tasks in a single DAG was 10 000 (!). The number of tasks\ndepends on the purposes of a DAG, so it’s rather hard to say if users have “simple”\nor “complicated” workflows.\n\n**When onboarding new members to Airflow, what is the biggest problem?**\n\n|                                                               |No.|  %   |\n|----------------------------------------------------","url":"airflow-survey","title":"Airflow Survey 2019","linkTitle":"Airflow Survey 2019","author":"Tomek Urbaszek","twitter":"Nuclearriot","github":"nuclearpinguin","linkedin":"tomaszurbaszek","description":"Receiving and adjusting to our users’ feedback is a must. Let’s see who Airflow users are, how they play with it, and what they miss.","tags":["community","survey","users"],"date":"2019-12-11"},{"content":"The brand [new Airflow website](https://airflow.apache.org/) has arrived! Those who have been following the process know that the journey to update [the old Airflow website](https://airflow.readthedocs.io/en/1.10.6/) started at the beginning of the year.\nThanks to sponsorship from the Cloud Composer team at Google that allowed us to\ncollaborate with [Polidea](https://www.polidea.com/) and with their design studio [Utilo](https://utilodesign.com/), and deliver an awesome website.\n\nDocumentation of open source projects is key to engaging new contributors in the maintenance,\ndevelopment, and adoption of software. We want the Apache Airflow community to have\nthe best possible experience to contribute and use the project. We also took this opportunity to make the project\nmore accessible, and in doing so, increase its reach.\n\nIn the past three and a half months, we have updated everything: created a more efficient landing page,\nenhanced information architecture, and improved UX & UI. Most importantly, the website now has capabilities\nto be translated into many languages. This is our effort to foster a more inclusive community around\nApache Airflow, and we look forward to seeing contributions in Spanish, Chinese, Russian, and other languages as well!\n\nWe built our website on Docsy, a platform that is easy to use and contribute to. Follow\n[these steps](https://github.com/apache/airflow-site/blob/aip-11/README.md) to set up your environment and\nto create your first pull request. You may also use\nthe new website for your own open source project as a template.\nAll of our [code is open and hosted on Github](https://github.com/apache/airflow-site/tree/aip-11).\n\nShare your questions, comments, and suggestions with us, to help us improve the website.\nWe hope that this new design makes finding documentation about Airflow easier,\nand that its improved accessibility increases adoption and use of Apache Airflow around the world.\n\nHappy browsing!\n","url":"announcing-new-website","title":"New Airflow website","linkTitle":"New Airflow website","author":"Aizhamal Nurmamat kyzy","description":"We are thrilled about our new website!","tags":["Community"],"date":"2019-12-11"},{"content":"Is it possible to create an organization that delivers tens of projects used by millions, nearly no one is paid for doing their job, and still, it has been fruitfully carrying on for more than 20 years? Apache Software Foundation proves it is possible. For the last two decades, ASF has been crafting a model called the Apache Way—a way of organizing and leading tech open source projects. Due to this approach, which is strongly based on the “community over code” motto, we can enjoy such awesome projects like Apache Spark, Flink, Beam, or Airflow (and many more).\n\nAfter this year’s ApacheCon, Polidea’s engineers talked with Committers of Apache projects, such as—Aizhamal Nurmamat kyzy, Felix Uellendall, and Fokko Driesprong—about insights to what makes the ASF such an amazing organization.\n\nYou can read the [insights on the Polidea blog](https://www.polidea.com/blog/apachecon-europe-2019-thoughts-and-insights-by-airflow-committers/?utm_source=ApacheAirflowBlog&utm_medium=Npaid&utm_campaign=Blog&utm_term=Article&utm_content=AAB_NOP_BLG_ART_APC_001).\n","url":"apache-con-europe-2019-thoughts-and-insights-by-airflow-committers","title":"ApacheCon Europe 2019 — Thoughts and Insights by Airflow Committers","linkTitle":"ApacheCon Europe 2019 — Thoughts and Insights by Airflow Committers","author":"Polidea","description":"Here come some thoughts by Airflow committers and contributors from the ApacheCon Europe 2019. Get to know the ASF community!","tags":["Community"],"date":"2019-11-22"},{"content":"## Documenting local development environment of Apache Airflow\n\nFrom Sept to November, 2019 I have been participating in a wonderful initiative, [Google Season of Docs](https://developers.google.com/season-of-docs).\n\nI had a pleasure to contribute to the Apache Airflow open source project as a technical writer.\nMy initial assignment was an extension to the github-based Contribution guide.\n\nFrom the very first days I have been pretty closely involved into inter-project communications\nvia emails/slack and had regular 1:1s with my mentor, Jarek Potiuk.\n\nI got infected with Jarek’s enthusiasm to ease the on-boarding experience for\nAirflow contributors. I do share this strategy and did my best to improve the structure,\nlanguage and DX. As a result, Jarek and I extended the current contributor’s docs and\nended up with the Contributing guide navigating the users through the project\ninfrastructure and providing a workflow example based on a real-life use case;\nthe Testing guide with an overview of a complex testing infrastructure for Apache Airflow;\nand two guides dedicated to the Breeze dev environment and local virtual environment\n(my initial assignment).\n\nI’m deeply grateful to my mentor and Airflow developers for their feedback,\npatience and help while I was breaking through new challenges\n(I’ve never worked on an open source project before),\nand for their support of all my ideas! I think a key success factor for any contributor\nis a responsive, supportive and motivated team, and I was lucky to join such\na team for 3 months.\n\nDocuments I worked on:\n\n* [Breeze development environment documentation](https://github.com/apache/airflow/blob/master/BREEZE.rst)\n* [Local virtualenv environment documentation](https://github.com/apache/airflow/blob/master/LOCAL_VIRTUALENV.rst)\n* [Contributing guide](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst)\n* [Testing guide](https://github.com/apache/airflow/blob/master/TESTING.rst)\n","url":"documenting-using-local-development-environments","title":"Documenting using local development environment","linkTitle":"Documenting using local development environment","author":"Elena Fedotova","github":"efedotova","linkedin":"elena-fedotova-039294110","description":"The story behind documenting local development environment of Apache Airflow","tags":["Development"],"date":"2019-11-22T00:00:00.000Z"},{"content":"I came across [Google Season of Docs][1] (GSoD) almost by accident, thanks to my extensive HackerNews and Twitter addiction.  I was familiar with the Google Summer of Code but not with this program.\nIt turns out it was the inaugural phase. I read the details, and the process felt a lot like GSoC except that this was about documentation.\n\n## About Me\nI have been writing tech articles on medium as well as my blog for the past 1.5 years.  Blogging helps me test my understanding of the concepts as untangling the toughest of ideas in simple sentences requires a considerable time investment.\n\nAlso, I have been working as a Software Developer for the past three years, which involves writing documentation for my projects as well. I completed my B.Tech from  IIT Roorkee. During my stay in college, I applied for GSoC once but didn’t make it through in the final list of selected candidates.\n\nI saw GSoD as an excellent opportunity to improve my technical writing skills using feedback from the open-source community. I contributed some bug fixes and features to Apache Superset and Apache Druid, but this would be my first contribution as a technical writer.\n\n## Searching for the organization\nAbout 40+ organizations were participating in the GSoD. However, there were two which came as the right choice for me in the first instant. The first one was [Apache Airflow][2] because I had already used Airflow extensively and also contributed some custom operators inside the forked version of my previous company.\n\nThe second one was [Apache Cassandra][3], on which I also had worked extensively but hadn’t done any code or doc changes.\n\nConsidering the total experience, I decided to go with the Airflow.\n\n## Project selection\nAfter selecting the org, the next step was to choose the project. Again, my previous experience played a role here, and I ended up picking the **How to create a workflow** . The aim of the project was to write documentation which will help users in creating complex as well as custom DAGs.  \nThe final deliverables were a bit different, though. More on that later.\n\nAfter submitting my application, I got involved in my job until one day, I saw a mail from google confirming my selection as a Technical Writer for the project.\n\n## Community Bonding\nGetting selected is just a beginning.  I got the invite to the Airflow slack channel where most of the discussions happened.\nMy mentor was [Ash-Berlin Taylor][4] from Apache Airflow. I started talking to my mentor to get a general sense of what deliverables were expected. The deliverables were documented in [confluence][5].\n\n- A page for how to create a DAG that also includes:\n    - Revamping the page related to scheduling a DAG\n    - Adding tips for specific DAG conditions, such as rerunning a failed task\n- A page for developing custom operators that includes:\n    - Describing mechanisms that are important when creating an operator, such as template fields, UI color, hooks, connection, etc.\n    - Describing the r","url":"experience-in-google-season-of-docs-2019-with-apache-airflow","title":"Experience in Google Season of Docs 2019 with Apache Airflow","linkTitle":"Experience in Google Season of Docs 2019 with Apache Airflow","author":"Kartik Khare","twitter":"khare_khote","github":"KKcorps","linkedin":"kharekartik","description":"","tags":["Documentation"],"date":"2019-12-20T00:00:00.000Z"}]